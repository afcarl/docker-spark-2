{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f41c005a990>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imported spark variables\n",
    "spark # the spark session\n",
    "sc # the spark context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important urls\n",
    "\n",
    "- [Spark Web UI](http://localhost:4040/)\n",
    "- [Spark Master UI](http://localhost:8080/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in which directory you are running, since metastore_db will be created there. There can be only one spark applicaton directory because metastore_db will be locked.\n",
    "The variables spark and sc are automatically imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/scripts/notebooks/n01\n",
      "example01.ipynb  gen_test_data.py  test_data.txt\n",
      "spark://2da1f8fedff5:7077\n",
      "derby.log  example01.ipynb  gen_test_data.py  metastore_db  test_data.txt\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls\n",
    "print sc.master\n",
    "spark.createDataFrame([], StructType([])) #force createDataFrame to create metastore_db and derby.log\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "Check out [Spark SQL Programming Guild](http://spark.apache.org/docs/latest/sql-programming-guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '/root/scripts/notebooks/n01/test_data.txt'\n",
    "\n",
    "def parse_line(line):\n",
    "    line_parts = [part.strip() for part in line.split(',')]\n",
    "    return Row(key=line_parts[0], value=line_parts[1])\n",
    "\n",
    "lines_rdd = sc. \\\n",
    "            textFile(filename). \\\n",
    "            map(parse_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(key=u'key_8738', value=u'value_298'),\n",
       " Row(key=u'key_8958', value=u'value_837')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_schema():\n",
    "    key_field = StructField('key', StringType(), nullable=False) \n",
    "    value_field = StructField('key', StringType(), nullable=False) \n",
    "    return StructType([key_field, value_field])\n",
    "\n",
    "schema=get_schema()\n",
    "key_values=spark.createDataFrame(lines_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|     key|    value|\n",
      "+--------+---------+\n",
      "|key_8738|value_298|\n",
      "|key_8958|value_837|\n",
      "+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "key_values.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# before we save we repartition the data, otherwise no parallelism\n",
    "key_values.repartition(4).write.parquet('key_values.parquet', mode='overwrite')\n",
    "del key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\r\n",
      "part-r-00000-c6dbe4c3-0861-4178-b5a9-50777eb27c74.snappy.parquet\r\n",
      "part-r-00001-c6dbe4c3-0861-4178-b5a9-50777eb27c74.snappy.parquet\r\n",
      "part-r-00002-c6dbe4c3-0861-4178-b5a9-50777eb27c74.snappy.parquet\r\n",
      "part-r-00003-c6dbe4c3-0861-4178-b5a9-50777eb27c74.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "#notice the 4 partitions\n",
    "!ls key_values.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|     key|num_of_values|\n",
      "+--------+-------------+\n",
      "|key_6770|            7|\n",
      "|key_3921|            7|\n",
      "|key_4540|            6|\n",
      "|key_5040|            6|\n",
      "|key_7186|            6|\n",
      "| key_393|            6|\n",
      "|key_1036|            5|\n",
      "|key_9636|            5|\n",
      "|key_7429|            5|\n",
      "|  key_27|            5|\n",
      "+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "key_values = spark.read.parquet('key_values.parquet')\n",
    "key_values.createOrReplaceTempView('key_values')\n",
    "top_keys = spark.sql(\"\"\"\n",
    "SELECT  key, \n",
    "        COUNT(*) AS num_of_values\n",
    "FROM key_values\n",
    "GROUP BY key\n",
    "ORDER BY num_of_values DESC\n",
    "LIMIT 10\n",
    "\"\"\")\n",
    "top_keys.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python27]",
   "language": "python",
   "name": "conda-env-python27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
